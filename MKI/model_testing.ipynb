{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af188011",
   "metadata": {},
   "source": [
    "AI Prediction testing with mediapipe coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3d2d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afaa5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the data, or locate\n",
    "\n",
    "# Extract frames and label them\n",
    "def extract_skeleton(video):\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose  = mp_pose.Pose(static_image_mode=False)\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    keypoints_sequence = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = pose.process(rgb)\n",
    "\n",
    "        if result.pose_landmarks:\n",
    "            landmarks = result.pose_landmarks.landmark\n",
    "            keypoints = []\n",
    "            for lm in landmarks:\n",
    "                keypoints.extend((lm.x, lm.y, lm.z, lm.visibility))\n",
    "            if len(keypoints) != 132:\n",
    "                keypoints = [0.0] * 132\n",
    "        else:\n",
    "            keypoints = [0.0] * 132\n",
    "\n",
    "        keypoints_sequence.append(keypoints)\n",
    "    \n",
    "    cap.release()\n",
    "    pose.close()\n",
    "    return np.array(keypoints_sequence)\n",
    "\n",
    "# (Collect the angles that we thought about, might be useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e7d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(folder_path):\n",
    "    X, y = [], []\n",
    "    label_counts = Counter()\n",
    "    video_stats = []\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if not file.lower().endswith(\".mp4\"):\n",
    "                continue\n",
    "\n",
    "            video_path = os.path.join(root, file)\n",
    "            label = 1 if \"enter\" in file.lower() else 0\n",
    "            skeleton = extract_skeleton(video_path)\n",
    "\n",
    "            num_frames = skeleton.shape[0]\n",
    "            video_stats.append((file, num_frames, label))\n",
    "            if num_frames == 0:\n",
    "                print(f\"Warning: No frames extracted for {file}\")\n",
    "                continue\n",
    "\n",
    "            feature = skeleton.flatten()\n",
    "            X.append(feature)\n",
    "            y.append(label)\n",
    "            label_counts[label] += 1\n",
    "\n",
    "    print(\"Label Distribution:\", label_counts)\n",
    "    print(\"Frame stats per video:\")\n",
    "    for name, frames, label in video_stats:\n",
    "        print(f\"{name}: {frames} frames, label: {label}\")\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e20a008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_preds = rf.predict(X_test)\n",
    "    print(\"Random Forest:\")\n",
    "    print(classification_report(y_test, rf.predict(X_test)))\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_preds = knn.predict(X_test)\n",
    "    print(\"KNN:\")\n",
    "    print(classification_report(y_test, knn.predict(X_test)))\n",
    "\n",
    "    joblib.dump(rf, 'rf_model.pkl')\n",
    "    joblib.dump(knn, 'knn_model.pkl')\n",
    "\n",
    "    return X_test, y_test, rf_preds, knn_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28b6a395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution: Counter({0: 80, 1: 79})\n",
      "Frame stats per video:\n",
      "front_enter_10_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_11_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_11_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_12_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_12_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_13_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_13_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_14_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_14_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_15_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_15_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_16_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_16_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_17_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_17_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_18_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_18_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_19_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_19_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_1_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_1_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_20_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_20_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_2_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_2_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_3_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_3_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_4_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_4_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_5_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_5_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_6_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_6_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_7_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_7_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_8_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_8_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_enter_9_trimmed.mp4: 20 frames, label: 1\n",
      "front_enter_9_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_10_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_10_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_11_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_11_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_12_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_12_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_13_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_13_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_14_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_14_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_15_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_15_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_16_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_16_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_17_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_17_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_18_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_18_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_19_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_19_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_1_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_1_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_20_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_20_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_2_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_2_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_3_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_3_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_4_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_4_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_5_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_5_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_6_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_6_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_7_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_7_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_8_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_8_trimmed_1.mp4: 20 frames, label: 1\n",
      "side_enter_9_trimmed.mp4: 20 frames, label: 1\n",
      "side_enter_9_trimmed_1.mp4: 20 frames, label: 1\n",
      "front_pass_10_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_10_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_11_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_11_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_12_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_12_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_13_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_13_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_14_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_14_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_15_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_15_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_16_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_16_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_17_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_17_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_18_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_18_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_19_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_19_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_1_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_1_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_20_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_20_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_2_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_2_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_3_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_3_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_4_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_4_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_5_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_5_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_6_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_6_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_7_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_7_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_8_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_8_trimmed_1.mp4: 20 frames, label: 0\n",
      "front_pass_9_trimmed.mp4: 20 frames, label: 0\n",
      "front_pass_9_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_10_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_10_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_11_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_11_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_12_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_12_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_13_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_13_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_14_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_14_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_15_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_15_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_16_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_16_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_17_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_17_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_18_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_18_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_19_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_19_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_1_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_1_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_20_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_20_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_2_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_2_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_3_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_3_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_4_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_4_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_5_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_5_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_6_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_6_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_7_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_7_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_8_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_8_trimmed_1.mp4: 20 frames, label: 0\n",
      "side_pass_9_trimmed.mp4: 20 frames, label: 0\n",
      "side_pass_9_trimmed_1.mp4: 20 frames, label: 0\n",
      "Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        15\n",
      "           1       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.91        32\n",
      "   macro avg       0.92      0.91      0.91        32\n",
      "weighted avg       0.92      0.91      0.91        32\n",
      "\n",
      "KNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.73      0.79        15\n",
      "           1       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.81        32\n",
      "   macro avg       0.82      0.81      0.81        32\n",
      "weighted avg       0.82      0.81      0.81        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trimmed_videos_path = \"C:/Users/hanna/Documents/Thesis/datainsamling/data/processed_videos/\"\n",
    "X, y = build_dataset(trimmed_videos_path)\n",
    "X_test, y_test, rf_preds, knn_preds = train_and_evaluate_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52527437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positives for RF (predicted 'enter' but actually 'pass'): 0\n",
      "False Negatives for RF (predicted 'pass' but actually 'enter'): 1\n",
      "False Positives for KNN (predicted 'enter' but actually 'pass'): 4\n",
      "False Negatives for KNN (predicted 'pass' but actually 'enter'): 1\n"
     ]
    }
   ],
   "source": [
    "# rf false classifications\n",
    "\n",
    "# False positives\n",
    "rf_fp_indices = np.where((rf_preds == 1) & (y_test == 0))[0]\n",
    "\n",
    "# False negatives\n",
    "rf_fn_indices = np.where((rf_preds == 0) & (y_test == 1))\n",
    "\n",
    "print(f\"False Positives for RF (predicted 'enter' but actually 'pass'): {len(rf_fp_indices)}\")\n",
    "print(f\"False Negatives for RF (predicted 'pass' but actually 'enter'): {len(rf_fn_indices)}\")\n",
    "\n",
    "\n",
    "# knn false classifications\n",
    "\n",
    "# False positives\n",
    "knn_fp_indices = np.where((knn_preds == 1) & (y_test == 0))[0]\n",
    "\n",
    "# False negatives\n",
    "knn_fn_indices = np.where((knn_preds == 0) & (y_test == 1))\n",
    "\n",
    "print(f\"False Positives for KNN (predicted 'enter' but actually 'pass'): {len(knn_fp_indices)}\")\n",
    "print(f\"False Negatives for KNN (predicted 'pass' but actually 'enter'): {len(knn_fn_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5741edf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "def build_timeseries_dataset(folder_path, max_seq_len=100):\n",
    "    X, y = [], []\n",
    "    label_counts = Counter()\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if not file.lower().endswith(\".mp4\"):\n",
    "                continue\n",
    "            video_path = os.path.join(root, file)\n",
    "            label = 1 if \"enter\" in os.path.basename(root).lower() else 0\n",
    "            skeleton = extract_skeleton(video_path)\n",
    "\n",
    "            # Pad or truncate\n",
    "            if skeleton.shape[0] < max_seq_len:\n",
    "                pad_len = max_seq_len - skeleton.shape[0]\n",
    "                padding = np.zeros((pad_len, skeleton.shape[1]))\n",
    "                skeleton = np.vstack([skeleton, padding])\n",
    "            else:\n",
    "                skeleton = skeleton[:max_seq_len]\n",
    "\n",
    "            X.append(skeleton)\n",
    "            y.append(label)\n",
    "            label_counts[label] += 1\n",
    "    \n",
    "    print(\"Label distribution: \", label_counts)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1a1bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fa92741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=1, num_classes=2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e24427ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(X, y, epochs=20, batch_size=16, lr=0.001):\n",
    "    dataset = PoseDataset(X,y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    input_size = X.shape[2]\n",
    "    model = LSTMClassifier(input_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16cdeb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lstm(model, X, y):\n",
    "    dataset = PoseDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            outputs = model(batch_x)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "133f08b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:  Counter({0: 80, 1: 79})\n",
      "Epoch 1/20, Loss: 0.6944\n",
      "Epoch 2/20, Loss: 0.6935\n",
      "Epoch 3/20, Loss: 0.6935\n",
      "Epoch 4/20, Loss: 0.6932\n",
      "Epoch 5/20, Loss: 0.6935\n",
      "Epoch 6/20, Loss: 0.6940\n",
      "Epoch 7/20, Loss: 0.6932\n",
      "Epoch 8/20, Loss: 0.6936\n",
      "Epoch 9/20, Loss: 0.6934\n",
      "Epoch 10/20, Loss: 0.6933\n",
      "Epoch 11/20, Loss: 0.6931\n",
      "Epoch 12/20, Loss: 0.6938\n",
      "Epoch 13/20, Loss: 0.6934\n",
      "Epoch 14/20, Loss: 0.6932\n",
      "Epoch 15/20, Loss: 0.6932\n",
      "Epoch 16/20, Loss: 0.6934\n",
      "Epoch 17/20, Loss: 0.6936\n",
      "Epoch 18/20, Loss: 0.6933\n",
      "Epoch 19/20, Loss: 0.6936\n",
      "Epoch 20/20, Loss: 0.6936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67        80\n",
      "           1       0.00      0.00      0.00        79\n",
      "\n",
      "    accuracy                           0.50       159\n",
      "   macro avg       0.25      0.50      0.33       159\n",
      "weighted avg       0.25      0.50      0.34       159\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hanna\\Documents\\Thesis\\exjobb\\venv-310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\hanna\\Documents\\Thesis\\exjobb\\venv-310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\hanna\\Documents\\Thesis\\exjobb\\venv-310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "trimmed_videos_path = \"C:/Users/hanna/Documents/Thesis/datainsamling/data/processed_videos/\"\n",
    "X, y = build_timeseries_dataset(trimmed_videos_path, max_seq_len=100)\n",
    "model = train_lstm(X, y, epochs=20)\n",
    "evaluate_lstm(model, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
